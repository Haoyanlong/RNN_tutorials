{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable as V\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generated mask according sequence_length and max length, 1 ---> truth, 0 ---> padded\n",
    "def sequence_mask(sequence_length, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.data.max()\n",
    "    batch_size = sequence_length.size(0)\n",
    "    seq_range = t.arange(0, max_len).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
    "    seq_range_expand = V(seq_range_expand)\n",
    "    if sequence_length.is_cuda:\n",
    "        seq_range_expand = seq_range_expand.cuda()\n",
    "    seq_length_expand = (sequence_length.unsqueeze(1).expand_as(seq_range_expand))\n",
    "    return seq_range_expand < seq_length_expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence length:  Variable containing:\n",
      " 9\n",
      " 8\n",
      " 7\n",
      " 6\n",
      " 5\n",
      " 4\n",
      " 3\n",
      " 2\n",
      " 1\n",
      "[torch.LongTensor of size 9]\n",
      "\n",
      "mask:  Variable containing:\n",
      "    1     1     1     1     1     1     1     1     1\n",
      "    1     1     1     1     1     1     1     1     0\n",
      "    1     1     1     1     1     1     1     0     0\n",
      "    1     1     1     1     1     1     0     0     0\n",
      "    1     1     1     1     1     0     0     0     0\n",
      "    1     1     1     1     0     0     0     0     0\n",
      "    1     1     1     0     0     0     0     0     0\n",
      "    1     1     0     0     0     0     0     0     0\n",
      "    1     0     0     0     0     0     0     0     0\n",
      "[torch.ByteTensor of size 9x9]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seqs_length = V(t.LongTensor([9, 8, 7, 6, 5, 4, 3, 2, 1]))\n",
    "print(\"sequence length: \", seqs_length)\n",
    "\n",
    "# generated mask\n",
    "mask = sequence_mask(seqs_length)\n",
    "print(\"mask: \", mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def masked_cross_entropy(logits, target, length):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: A Variable containing a FloatTensor of size (batch, max_len, num_classes)\n",
    "                which contains the unnormalized probability for each class.\n",
    "        \n",
    "        target: A Variable containing a LongTensor of size (batch, max_len) which contains\n",
    "                the index of the true class for each corresponding step.\n",
    "        \n",
    "        length: A Variable containing a LongTensor of size (batch,) which contains the length\n",
    "                of each data in a batch.\n",
    "                \n",
    "    Returns:\n",
    "        loss: An average loss value masked by the length.\n",
    "    \n",
    "    \"\"\"\n",
    "    length = V(t.LongTensor(length))\n",
    "    # if t.cuda.is_available():\n",
    "    #     length = length.cuda()\n",
    "    \n",
    "    # logits_flat: (batch * max_len, num_classes)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    # log_probs_flat: (batch * max_len, num_classes)\n",
    "    log_probs_flat = F.log_softmax(logits_flat)\n",
    "    \n",
    "    # target_flat: (batch * max_len, 1)\n",
    "    target_flat = target.view(-1, 1)\n",
    "    \n",
    "    # losses_flat: (batch * max_len, 1)\n",
    "    losses_flat = -t.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "    # losses: (batch, max_len)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    \n",
    "    # mask: (batch, max_len)\n",
    "    mask = sequence_mask(sequence_length=length, max_len=target.size(1))\n",
    "    \n",
    "    losses = losses * mask.float()\n",
    "    loss = losses.sum() / length.float().sum()\n",
    "    return loss   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
