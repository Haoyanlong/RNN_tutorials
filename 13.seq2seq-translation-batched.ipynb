{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import jieba\n",
    "import unicodedata\n",
    "import codecs\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable as V\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, models\n",
    "from torchvision import transforms as T\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Load data\n",
    "\n",
    "#### indexing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "class Lang:\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2count = {}\n",
    "        self.word2index = {}\n",
    "        self.index2word = {0: \"PAD\", 1:\"SOS\", 2:\"EOS\"}\n",
    "        self.n_words = 3  # Count default tokens\n",
    "    \n",
    "    \n",
    "    def index_words(self, sentence, chi=False):\n",
    "        if chi:\n",
    "            for word in list(jieba.cut(sentence)):\n",
    "                self.index_word(word)\n",
    "        else:        \n",
    "            for word in sentence.split(' '):\n",
    "                self.index_word(word)\n",
    "    \n",
    "    def index_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "    \n",
    "    # Remove words below a certain count threshold\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed: \n",
    "            return \n",
    "        # self.trimmed = True\n",
    "        \n",
    "        keep_words = []\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "        \n",
    "        print(\"keep_words %d / %d = %.4f\" % \\\n",
    "              (len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index) ))\n",
    "        \n",
    "        # Reinitialize dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\"}\n",
    "        self.n_words = 3       # Count default tokens\n",
    "        for word in keep_words:\n",
    "            self.index_word(word)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading and decoding files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([,.!?])\", r\" \\1 \", s)\n",
    "    s = re.sub(r\"[^a-zA-Z,.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_langs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "    \n",
    "    # Read the file and split into lines\n",
    "    filename = './data/eng-chi/%s-%s.txt' % (lang1, lang2)\n",
    "    # lines = codecs.open(filename, 'r', 'utf-8').read().strip().split('\\n')\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Split every line into pairs and normalize\n",
    "    # pairs = [[normalize_string(s) for s in l.split('\\t')] for l in lines]\n",
    "    pairs = [[s for s in l.split('\\t')] for l in lines]\n",
    "    \n",
    "    \n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "    \n",
    "    return input_lang, output_lang, pairs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = read_langs('eng', 'chi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MIN_LENGTH = 5\n",
    "MAX_LENGTH = 15\n",
    "\n",
    "def filter_pairs(pairs):\n",
    "    filtered_pairs = []\n",
    "    for pair in pairs:\n",
    "        if len(pair[0].split(' ')) >= MIN_LENGTH and len(pair[0].split(' ')) <= MAX_LENGTH \\\n",
    "         and len(list(jieba.cut(pair[1]))) >= MIN_LENGTH and len(list(jieba.cut(pair[1]))) <= MAX_LENGTH:\n",
    "            filtered_pairs.append(pair)\n",
    "    return filtered_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(lang1_name, lang2_name, reverse=False):\n",
    "    input_lang, output_lang, pairs = read_langs(lang1_name, lang2_name, reverse)\n",
    "    print(\"Read %d sentence pairs\" % len(pairs))\n",
    "    \n",
    "    pairs = filter_pairs(pairs)\n",
    "    print(\"Filtered to %d pairs\" % len(pairs))\n",
    "    \n",
    "    print(\"Indexing words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.index_words(pair[0], chi=False)\n",
    "        output_lang.index_words(pair[1], chi=True)\n",
    "    print(\"Indexed %d words in input language, %d words in output\" % \n",
    "          (input_lang.n_words, output_lang.n_words))\n",
    "    return input_lang, output_lang, pairs\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.287 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 19777 sentence pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered to 15687 pairs\n",
      "Indexing words...\n",
      "Indexed 9318 words in input language, 12208 words in output\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepare_data('eng', 'chi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter words \n",
    "\n",
    "If don't filter words, there are 3226 words in input language, 3438 words in output language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep_words 4831 / 9315 = 0.5186\n",
      "keep_words 5222 / 12205 = 0.4279\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT= 2    # the word at least occurs times\n",
    "\n",
    "input_lang.trim(MIN_COUNT)\n",
    "output_lang.trim(MIN_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input language words:  4834\n",
      "output language words:  5225\n"
     ]
    }
   ],
   "source": [
    "print(\"input language words: \", input_lang.n_words)\n",
    "print(\"output language words: \", output_lang.n_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering pairs\n",
    "\n",
    "Now we will go back to the set of all sentence pairs and remove those with unknown words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed from 15687 pairs to 8942, 0.5700 of total\n"
     ]
    }
   ],
   "source": [
    "# print(\"pairs length: \", len(pairs))\n",
    "keep_pairs = []\n",
    "\n",
    "for pair in pairs:\n",
    "    input_sentence = pair[0]\n",
    "    output_sentence = pair[1]\n",
    "    \n",
    "    keep_input = True\n",
    "    keep_output = True\n",
    "    \n",
    "    for word in input_sentence.split(' '):\n",
    "        if word not in input_lang.word2index:\n",
    "            keep_input = False\n",
    "            break\n",
    "\n",
    "\n",
    "    for word in list(jieba.cut(output_sentence)):\n",
    "        if word not in output_lang.word2index:\n",
    "            keep_output = False\n",
    "            break\n",
    "    \n",
    "    if keep_input and keep_output:\n",
    "        keep_pairs.append(pair)\n",
    "print(\"Trimmed from %d pairs to %d, %.4f of total\" % \\\n",
    "      (len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original input(English):  I have to go to sleep.\n",
      "translate output(Chinese):  我该去睡觉了。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pair = random.choice(keep_pairs)\n",
    "print(\"original input(English): \", pair[0])\n",
    "print(\"translate output(Chinese): \", pair[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turning training data into Tensors\n",
    "\n",
    "To train we need to turn the sentences into something the neural network can understand, which of course means numbers. Each sentence will be split into words and turned into a LongTensor which represents the index (from the Lang indexes made earlier) of each word.While creating these tensors we will also append the EOS token to signal that the sentence is over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Return a list of indexes, one for each word in the sentence, plus EOS\n",
    "def indexes_from_sentence(lang, sentence, chi=False):\n",
    "    if chi:\n",
    "        return [lang.word2index[word] for word in list(jieba.cut(sentence))] + [EOS_token]\n",
    "    else:\n",
    "        return [lang.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make better use of the GPU by training on batches of many sequences at once, but doing so brings up the question of how to deal with sequences of varying lengths. The simple solution is to 'pad' the shorter sentences with some padding symbol(in this case 0), and ignore these padded spots when calculating the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pad a with the PAD symbol\n",
    "def pad_seq(seq, max_length):\n",
    "    seq += [PAD_token for i in range(max_length - len(seq))]\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * *\n",
    "To create a Variable for a full batch of inputs(and targets) we get a random sample of sequences and pad them all to the length of the longest sequence.We'll keep track of the lengths of each batch in order to un-pad later.\n",
    "\n",
    "Initializing a LongTensor with an array (batches) of arrays (sequences) gives us a (batch_size \\* max_len) tensor - selecting the first dimension gives you a simple batch, which is a full sequence. When training the model we'll want a simple time step at once, so we'll transpose (max_len \\* batch_size). Now selecting along the first dimension returns a single time step across batches.\n",
    "* * *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input lang:  <__main__.Lang instance at 0x7f52e1f2d440>\n",
      "output lang:  <__main__.Lang instance at 0x7f52e1f2d4d0>\n"
     ]
    }
   ],
   "source": [
    "print(\"input lang: \", input_lang)\n",
    "print(\"output lang: \", output_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def random_batch(batch_size):\n",
    "    input_seqs = []\n",
    "    target_seqs = []\n",
    "    \n",
    "    # Choose random pairs\n",
    "    for i in range(batch_size):\n",
    "        pair = random.choice(keep_pairs)\n",
    "        input_seqs.append(indexes_from_sentence(input_lang, pair[0], chi=False))\n",
    "        target_seqs.append(indexes_from_sentence(output_lang, pair[1], chi=True))\n",
    "    \n",
    "    # Zip into pairs, sort by length (descending), unzip\n",
    "    seq_pairs = sorted(zip(input_seqs, target_seqs), key=lambda p: len(p[0]), reverse=True)\n",
    "    input_seqs, target_seqs = zip(*seq_pairs)\n",
    "    \n",
    "    # For input and target sequences, get array of lengths and pad with 0s to max length\n",
    "    input_lengths = map(len, input_seqs)\n",
    "    input_max_length = max(input_lengths)\n",
    "    input_padded = [pad_seq(s, input_max_length) for s in input_seqs]\n",
    "    \n",
    "    \n",
    "    target_lengths = map(len, target_seqs)\n",
    "    target_max_length = max(target_lengths)\n",
    "    target_padded = [pad_seq(s, target_max_length) for s in target_seqs]\n",
    "    \n",
    "    # Turn padded arrays into (batch_size * max_len) tensors, transpose into (max_len * batch_size)\n",
    "    input_var = V(t.LongTensor(input_padded)).transpose(0, 1)\n",
    "    target_var = V(t.LongTensor(target_padded)).transpose(0, 1)\n",
    "    \n",
    "    # if t.cuda.is_available():\n",
    "    #     input_var = input_var.cuda()\n",
    "    #     target_var = target_var.cuda()\n",
    "    return input_var, input_lengths, target_var, target_lengths\n",
    "      \n",
    "input_var, input_lengths, target_var, target_lengths = random_batch(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input variable:  Variable containing:\n",
      " 2376  1804  3902\n",
      " 4243  3063  2612\n",
      " 3308   654   943\n",
      " 2805   625   654\n",
      " 3606   615  1418\n",
      " 3663  1762  4223\n",
      "  424     2     2\n",
      "    2     0     0\n",
      "[torch.LongTensor of size 8x3]\n",
      "\n",
      "input lengths:  [8, 7, 7]\n",
      "target variable:  Variable containing:\n",
      " 4882  3768  3284\n",
      " 3072  3868  4467\n",
      "  492   500   825\n",
      " 2010  3927  5080\n",
      " 2779  2962  4683\n",
      " 4911  4673  4673\n",
      " 2553  3263  3263\n",
      " 4673     2     2\n",
      " 3263     0     0\n",
      "    2     0     0\n",
      "[torch.LongTensor of size 10x3]\n",
      "\n",
      "target lengths:  [10, 8, 8]\n"
     ]
    }
   ],
   "source": [
    "print(\"input variable: \", input_var)\n",
    "print(\"input lengths: \", input_lengths)\n",
    "print(\"target variable: \", target_var)\n",
    "print(\"target lengths: \", target_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Encoder\n",
    " * * * \n",
    "The encoder will take a batch of word sequences, a LongTensor of size (max_len \\* batch_size), and output an encoding for each word, a FloatTensor of size (max_len \\* batch_size \\* hidden_size)\n",
    "\n",
    "The word inputs are fed through an embedding layer nn.Embedding to create an embedding for each word, with size seq_len \\* hidden_size(as if it was a batch of words).This is resized to seq_len \\* 1 \\* hidden_size to fit the expected input of the GRU layer(nn.GRU).The GRU will return both an output sequence of size seq_len \\* hidden_size\n",
    "\n",
    "* * *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, batch_size, \n",
    "                 num_layers=1, dropout=0.1, bidirectional=True):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=input_size, \n",
    "                                      embedding_dim=hidden_size)\n",
    "        self.gru = nn.GRU(input_size=hidden_size, \n",
    "                          hidden_size=hidden_size, \n",
    "                          num_layers=num_layers, \n",
    "                          dropout=dropout, \n",
    "                          bidirectional=bidirectional)\n",
    "    \n",
    "    def forward(self, input_seqs, input_lengths, hidden=None):\n",
    "        # Note: we run this all at once (over multiple batches of multiple sequences)\n",
    "        print(\"input sequence size: \", input_seqs.size())\n",
    "        embedded = self.embedding(input_seqs)\n",
    "        print(\"embedded size: \", embedded.size())\n",
    "        \n",
    "        packed = pack_padded_sequence(embedded, input_lengths) \n",
    "        print(\"packed data size: \", packed.data.size())\n",
    "        print(\"packed batch_sizes: \", packed.batch_sizes)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        \n",
    "        outputs, output_lengths = pad_packed_sequence(outputs)  # unpack (back to padded)\n",
    "        print(\"outputs size: \", outputs.size())\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]   \n",
    "        return outputs, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return V(t.zeros(self.num_layers * (self.bidirectional + 1), self.batch_size, self.hidden_size))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder:  EncoderRNN(\n",
      "  (embedding): Embedding(4834, 23)\n",
      "  (gru): GRU(23, 23, dropout=0.1, bidirectional=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "INPUT_SIZE = input_lang.n_words\n",
    "HIDDEN_SIZE = 23\n",
    "BATCH_SIZE = 3\n",
    "# print(\"input size: \", INPUT_SIZE)\n",
    "# print(\"hidden size: \", HIDDEN_SIZE)\n",
    "\n",
    "encoder = EncoderRNN(input_size = INPUT_SIZE, \n",
    "                     hidden_size = HIDDEN_SIZE,\n",
    "                     batch_size = BATCH_SIZE,\n",
    "                     num_layers = 1,\n",
    "                     dropout = 0.1\n",
    "                     )\n",
    "print(\"encoder: \", encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_lengths:  [8, 7, 7]\n",
      "input sequence size:  torch.Size([8, 3])\n",
      "embedded size:  torch.Size([8, 3, 23])\n",
      "packed data size:  torch.Size([22, 23])\n",
      "packed batch_sizes:  [3, 3, 3, 3, 3, 3, 3, 1]\n",
      "outputs size:  torch.Size([8, 3, 46])\n"
     ]
    }
   ],
   "source": [
    "# print(\"input variable: \", input_var)\n",
    "print(\"input_lengths: \", input_lengths)\n",
    "\n",
    "encoder_hidden = encoder.init_hidden()\n",
    "# print(\"encoder hidden: \", encoder_hidden)\n",
    "\n",
    "outputs, hidden = encoder(input_var, input_lengths, encoder_hidden)\n",
    "\n",
    "# print(\"outputs size: \", outputs.size())\n",
    "# print(\"hidden size: \", hidden.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention Decoder\n",
    "\n",
    "##### Interpreting the Bahdanau et al.model\n",
    "\n",
    "Each decoder output is conditioned on the previous outputs and some x, where x consists of the current hidden state (which takes into account previous outputs) and the attention \n",
    "\"context\", which is calculated below.The function g is fully-connected layer with a nonlinear activation, which takes as input the values $y_{i-1}$, $s_i$, and $c_i$ concatenated.\n",
    "\n",
    "$$p(y_i|\\{y_1, ..., y_{i-1}\\}, x) = g(y_{i-1}, s_i, c_i)$$\n",
    "In the code, the RNN will be a nn.GRU layer, the hidden state $s_i$ will be called *hidden*, the output $y_i$ called *output*, and context $c_i$ called context.\n",
    "\n",
    "$$s_i = f(s_{i-1}, y_{i-1}, c_i)$$\n",
    "\n",
    "The context vector $c_i$ is a weighted sum of all encoder outputs, where each weight $\\alpha_{ij}$ is the the amount of \"attention\" paid to the corresponding encoder output $h_j$.\n",
    "\n",
    "$$c_i = \\sum_{j=1}^{T_x}\\alpha_{ij}h_j$$\n",
    "\n",
    "...where each weight $\\alpha_{ij}$ is a normalized (over all steps) attention \"energy\"$e_{ij}$...\n",
    "\n",
    "$$\\alpha_{ij}=\\frac{exp(e_{ij})}{\\sum_{k=1}^{T}exp(e_{ik})}$$\n",
    "\n",
    "...where each attention energy is calculated with some function $\\alpha$(such as another linear layer) using the last hidden state $s_{i-1}$ and that particular encoder output $h_j$\n",
    "\n",
    "$$e_{ij} = \\alpha(s_{i-1},h_j)$$\n",
    "\n",
    "\n",
    "##### Interpreting the Luong et al. models\n",
    "\n",
    "A few more attention models that offer improvements and simplifications.They describe a few \"global attention\" models, the distinction between them being the way the attention scores are calculated.\n",
    "\n",
    "The general form of the attention calculation relies on the target(decoder) side hidden state and corresponding source(encoder) side state, normalized over all states to get values summing to 1:\n",
    "\n",
    "$$\\alpha_t(s)=align(h_t, \\bar{h_s}) = \\frac{exp(score(h_t, \\bar{h_s}))}{\\sum_{s'}exp(score(h_t, \\bar{h_{s'}}))}$$\n",
    "\n",
    "The specific \"score\" function that compares two states is either *dot*, a simple dot product between the states; general, a dot product between the decoder hidden state and a linear transform of the encoder state; or concat, a dot product between a new parameter $v_{\\alpha}$ and a linear transform of the states concatenated together.\n",
    "\n",
    "$$\\begin{equation}\n",
    "score(h_t, \\bar{h_s}) = \\left\\{\n",
    "\\begin{aligned}\n",
    "h_t^T\\bar{h_s} & & dot    \\\\\n",
    "h_t^TW_{\\alpha}\\bar{h_s} & & general    \\\\\n",
    "v_{\\alpha}^TW_{\\alpha}[h_t;\\bar{h_s}] & & concat   \\\\\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "\\end{equation}$$\n",
    "\n",
    "The modular definition of these scoring functions gives us an opportunity to build specific attention module that can switch between the different score methods.\n",
    "The input to this module is always the hidden state (of the decoder RNN) and set of encoder outputs.\n",
    "\n",
    "#### Implementing an attention module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    \n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        if self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "            self.v = nn.Parameter(t.FloatTensor(1, self.hidden_size))\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \n",
    "        # hidden---> size: []\n",
    "        # encoder_outputs---> size: [10, 3, 46]([seq_len, batch_size, hidden_size])\n",
    "        max_len = encoder_outputs.size(0)\n",
    "        this_batch_size = encoder_outputs.size(1)\n",
    "        \n",
    "        # Create variable to store attention energies\n",
    "        attn_energies = V(t.zeros(this_batch_size, max_len))  # B * S\n",
    "        \n",
    "        # if t.cuda.is_available():\n",
    "        #     attn_energies = attn_energies.cuda()\n",
    "        for b in range(this_batch_size):\n",
    "            # Calculate energy for each encoder output\n",
    "            for i in range(max_len):            #              [1 * hidden_size]\n",
    "                attn_energies[b, i] = self.score(hidden[:, b], encoder_outputs[i, b].unsqueeze(0))\n",
    "        \n",
    "        # Normalize energies to weights in range 0 to 1, resize to 1 * B * S\n",
    "        return F.softmax(attn_energies).unsqueeze(1)\n",
    "\n",
    "    def score(self, hidden, encoder_output):\n",
    "        \n",
    "        if self.method == 'dot':\n",
    "            energy = hidden.dot(encoder_output)\n",
    "            return energy\n",
    "        elif self.method == 'general':\n",
    "            energy = self.attn(encoder_output)\n",
    "            energy = hidden.dot(energy)\n",
    "            return energy\n",
    "        elif self.method == 'concat':\n",
    "            energy = self.attn(t.cat((hidden, encoder_output), 1))\n",
    "            energy = self.v.dot(energy)\n",
    "            return energy  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Implementing the Bahdanau et al. model\n",
    "\n",
    "In summary our decoder should consist of four main parts - an embedding layer turning an input word into vector; a layer to calculate the attention energy per encoder output; a RNN layer; and an output layer.\n",
    "\n",
    "The decoder's inputs are the last RNN hidden state $s_{i-1}$, last output $y_{i-1}$, and all encoder outputs $h$\n",
    "\n",
    "- embedding layer with inputs $y_{i-1}$\n",
    "  - embedded = embedding(last_rnn_output)\n",
    "\n",
    "- attention layer $\\alpha$ with inputs ($s_{i-1}$, $h_j$) and outputs $e_{ij}$, normalized to create $\\alpha_{ij}$\n",
    "  - attn_energies[j] = attn_layer(last_hidden, encoder_outputs[j])\n",
    "  - attn_weights = normalize(attn_energies)\n",
    "\n",
    "- context vector $c_i$ as an attention-weighted average of encoder outputs\n",
    "  - context = sum(attn_weights * encoder_outputs)\n",
    "\n",
    "- RNN layer(s) f with inputs ($s_{i-1}$, $y_{i-1}$, $c_i$) and internal hidden state, outputting $s_i$\n",
    "  - rnn_input = concat(embedded, context)\n",
    "  - rnn_output, rnn_hidden = rnn(rnn_input, last_hidden)\n",
    "\n",
    "- an output layer g with inputs ($y_{i-1}$, $s_i$, $c_i$), outputting $y_i$\n",
    "  - output = out(embedded, rnn_output, context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BahdanauAttnDecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, output_size, num_layers=1, dropout_p=0.1):\n",
    "        super(BahdanauAttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        # Define parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(num_embeddings=output_size, \n",
    "                                      embedding_dim=hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.attn = Attn(method='concat', \n",
    "                         hidden_size=hidden_size)\n",
    "        self.gru = nn.GRU(input_size=hidden_size, \n",
    "                          hidden_size=hidden_size,\n",
    "                          num_layers=num_layers,\n",
    "                          dropout=dropout_p)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, word_input, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time\n",
    "        # TODO: FIX BATCHING\n",
    "        \n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        word_embedded = self.embedding(word_input).view(1, 1, -1)  # S = 1 * B * N\n",
    "        word_embedded = self.dropout(word_embedded)\n",
    "        \n",
    "        # Calculate attention weigths and apply to encoder outputs\n",
    "        attn_weights = self.attn(last_hidden[-1], encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B * 1 * N\n",
    "        context = context.transpose(0, 1)   # 1 * B * N\n",
    "        \n",
    "        # Combine embedded input word and attended context, run through RNN\n",
    "        rnn_input = t.cat((word_embedded, context), 2)\n",
    "        output, hidden = self.gru(rnn_input, last_hidden)\n",
    "        \n",
    "        # Final output layer\n",
    "        output = output.squeeze(0)\n",
    "        output = F.log_softmax(self.out(t.cat((output, context), 1)))\n",
    "        \n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden, attn_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build a decoder that plugs this Attn module in after the RNN to calculate attention weight, and apply those weights to the encoder output to get a context vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, attn_model, hidden_size, output_size, num_layers=1, dropout_p=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        # Define parameters\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.embedding_dropout = nn.Dropout(dropout_p)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers, dropout_p)\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # Choose attention model\n",
    "        if attn_model != None:\n",
    "            self.attn = Attn(attn_model, hidden_size)\n",
    "    \n",
    "    def forward(self, input_seq, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time\n",
    "        \n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        batch_size = input_seq.size(0)\n",
    "        embedded = self.embedding(input_seq)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        embedded = embedded.view(1, batch_size, self.hidden_size)    # S = 1 * B * N\n",
    "            \n",
    "        # Get current hidden state from input word and last hidden state\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        \n",
    "        # Calculate attention from current RNN state and all encoder outputs:\n",
    "        # apply to encoder outputs to get weighted average\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))  # B * (S=1) * N\n",
    "        \n",
    "        # Attention vector using the RNN hidden state and context vector\n",
    "        # concatenated together (Luong eq. 5)\n",
    "        rnn_output = rnn_output.squeeze(0)     # (S=1) * B * N  -----> B * N\n",
    "        context = context.squeeze(1)           # B * (S=1) * N  -----> B * N\n",
    "        concat_input = t.cat((rnn_output, context), 1)\n",
    "        \n",
    "        concat_output = F.tanh(self.concat(concat_input))\n",
    "        \n",
    "        # Finally predict next token （Luong eq. 6, without softmax)\n",
    "        output = self.out(concat_output)\n",
    "        \n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the models\n",
    "\n",
    "To make sure the encoder and decoder modules are working (and working together) we'll do a full test with a small batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input batches:  torch.Size([8, 3])\n",
      "target batches:  torch.Size([9, 3])\n"
     ]
    }
   ],
   "source": [
    "small_batch_size = 3\n",
    "\n",
    "input_batches, input_lengths, target_batches, target_lengths = random_batch(small_batch_size)\n",
    "\n",
    "print(\"input batches: \", input_batches.size())\n",
    "print(\"target batches: \", target_batches.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input seqs:  I want to travel with you. EOS PAD\n",
      "target seqs:  我想和你去旅行。\n",
      "EOS\n"
     ]
    }
   ],
   "source": [
    "print(\"input seqs: \", ' '.join([input_lang.index2word[eachnum] \\\n",
    "                                for eachnum in input_batches[:, 1].data.numpy().tolist()]))\n",
    "print(\"target seqs: \", ''.join([output_lang.index2word[eachnum] \\\n",
    "                                for eachnum in target_batches[:, 1].data.numpy().tolist()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create models with a small size(a good idea for eyeball inspection):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size:  4834\n",
      "output size:  5225\n",
      "encoder:  EncoderRNN(\n",
      "  (embedding): Embedding(4834, 8)\n",
      "  (gru): GRU(8, 8, num_layers=2, dropout=0.1, bidirectional=True)\n",
      ")\n",
      "decoder:  LuongAttnDecoderRNN(\n",
      "  (embedding): Embedding(5225, 8)\n",
      "  (embedding_dropout): Dropout(p=0.1)\n",
      "  (gru): GRU(8, 8, num_layers=2, bias=0.1)\n",
      "  (concat): Linear(in_features=16, out_features=8)\n",
      "  (out): Linear(in_features=8, out_features=5225)\n",
      "  (attn): Attn(\n",
      "    (attn): Linear(in_features=8, out_features=8)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "small_hidden_size = 8\n",
    "small_n_layers = 2\n",
    "\n",
    "input_size = input_lang.n_words\n",
    "output_size = output_lang.n_words\n",
    "print(\"input size: \", input_size)\n",
    "print(\"output size: \", output_size)\n",
    "\n",
    "encoder_test = EncoderRNN(input_size=input_size, \n",
    "                          hidden_size=small_hidden_size, \n",
    "                          batch_size=small_batch_size,\n",
    "                          num_layers=small_n_layers,\n",
    "                          dropout=0.1,\n",
    "                          bidirectional=True\n",
    "                          )\n",
    "decoder_test = LuongAttnDecoderRNN(attn_model='general', \n",
    "                                   hidden_size=small_hidden_size, \n",
    "                                   output_size=output_size, \n",
    "                                   num_layers=small_n_layers,\n",
    "                                   dropout_p=0.1)\n",
    "# if t.cuda.is_available():\n",
    "#     encoder_test.cuda()\n",
    "#     decoder_test.cuda()\n",
    "\n",
    "print(\"encoder: \", encoder_test)\n",
    "print(\"decoder: \", decoder_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the encoder, run the input batch through to get per-batch encoder outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sequence size:  torch.Size([8, 3])\n",
      "embedded size:  torch.Size([8, 3, 8])\n",
      "packed data size:  torch.Size([22, 8])\n",
      "packed batch_sizes:  [3, 3, 3, 3, 3, 3, 3, 1]\n",
      "outputs size:  torch.Size([8, 3, 16])\n",
      "***************************************************\n",
      "input_batches:  torch.Size([8, 3])\n",
      "encoder outputs:  torch.Size([8, 3, 8])\n",
      "encoder hidden:  torch.Size([4, 3, 8])\n"
     ]
    }
   ],
   "source": [
    "encoder_outputs, encoder_hidden = encoder_test(input_batches, input_lengths, hidden=None)\n",
    "print(\"***************************************************\")\n",
    "print(\"input_batches: \", input_batches.size())\n",
    "print(\"encoder outputs: \", encoder_outputs.size())   # max_len * batch_size * hidden_size\n",
    "print(\"encoder hidden: \", encoder_hidden.size())     # (n_layers * 2) * batch_size * hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then starting with a SOS token, run word tokens through the decoder to get each next word token. Instead of doing this with the whole sequence, it is done one at a time, to support using it's own predictions to make the next predition. This will be one time step at a time, but batched per time step. In order to get this to work for short padded sequences, the batch size is going to get smaller each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target max length:  9\n"
     ]
    }
   ],
   "source": [
    "max_target_length = max(target_lengths)\n",
    "print(\"target max length: \", max_target_length)\n",
    "\n",
    "\n",
    "# Prepare decoder input and outputs\n",
    "decoder_input = V(t.LongTensor([SOS_token] * small_batch_size))\n",
    "decoder_hidden = encoder_hidden[:decoder_test.num_layers] \n",
    "# Use last (forward) hidden state from encoder\n",
    "all_decoder_outputs = V(t.zeros(max_target_length, small_batch_size, decoder_test.output_size))\n",
    "\n",
    "if t.cuda.is_available():\n",
    "    all_decoder_outputs = all_decoder_outputs.cuda()\n",
    "    decoder_input = decoder_input.cuda()\n",
    "\n",
    "# Run through decoder one time step at a time\n",
    "for t in range(max_target_length):\n",
    "    decoder_output, decoder_hidden, decoder_attn = \\\n",
    "        decoder_test(decoder_input, decoder_hidden, encoder_outputs)\n",
    "    \n",
    "    all_decoder_outputs[t] = decoder_output   # Store this step's outputs\n",
    "    # Teacher forcing\n",
    "    decoder_input = target_batches[t]         # Next input is current target\n",
    "\n",
    "# Test masked cross entropy loss\n",
    "loss = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
