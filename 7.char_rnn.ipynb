{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable as V\n",
    "import torch.utils.data as Data\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from pyfile.text_loader import TextDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.parameters settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "n_layers = 3\n",
    "batch_size = 1\n",
    "n_epochs = 2\n",
    "n_characters = 128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embed = nn.Embedding(num_embeddings=input_size,\n",
    "                                  embedding_dim=hidden_size)\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=hidden_size, \n",
    "                          hidden_size=hidden_size, \n",
    "                          num_layers=n_layers)\n",
    "        \n",
    "        self.fc = nn.Linear(in_features=hidden_size, \n",
    "                            out_features=output_size)\n",
    "    \n",
    "    # This runs this one step at a time\n",
    "    # It's extremely slow, and please do not use in practice.\n",
    "    # We need to use(1) batch and (2) data parallelism\n",
    "    def forward(self, inputs, hidden):\n",
    "        embed = self.embed(inputs.view(1, -1))  # S(=1) * I\n",
    "        embed = embed.view(1, 1, -1)            # S(=1) * B(=1) * embedding_size\n",
    "        \n",
    "        outputs, hidden = self.gru(embed, hidden)\n",
    "        outputs = self.fc(outputs.view(1, -1))  # S(=1) * I\n",
    "        return outputs, hidden\n",
    "        \n",
    "    \n",
    "    def init_hidden(self):\n",
    "        hidden = V(t.zeros(self.n_layers, 1, hidden_size))\n",
    "        \n",
    "        if t.cuda.is_available():\n",
    "            return hidden.cuda()\n",
    "        else:\n",
    "            return hidden\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# help function\n",
    "def str2tensor(strings):\n",
    "    tensor = [ord(char) for char in strings]\n",
    "    tensor = t.LongTensor(tensor)\n",
    "    \n",
    "    if t.cuda.is_available():\n",
    "        tensor = tensor.cuda()\n",
    "    return V(tensor)\n",
    "\n",
    "\n",
    "def generate(decoder, prime_str='A', predict_len=100, temperature=0.8):\n",
    "    hidden = decoder.init_hidden()\n",
    "    prime_input = str2tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "    \n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = decoder(prime_input[p], hidden)\n",
    "        \n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = decoder(inp, hidden)\n",
    "        \n",
    "        # Note: 网络作为多元正太分布进行采样\n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = t.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = chr(top_i)\n",
    "        predicted += predicted_char\n",
    "        inp = str2tensor(predicted_char)\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train for a given src and target\n",
    "# It feeds single string to demonstrate seq2seq\n",
    "# It's extremely slow, and we need to use (1) batch and (2) data parallelism\n",
    "def train_teacher_forching(line):\n",
    "    inputs = str2tensor(line[:-1])\n",
    "    target = str2tensor(line[1:])\n",
    "    \n",
    "    hidden = decoder.init_hidden()\n",
    "    loss = 0\n",
    "    for c in range(len(inputs)):\n",
    "        output, hidden = decoder(inputs[c], hidden)\n",
    "        loss += criterion(output, target[c])\n",
    "        \n",
    "    decoder.zero_grad()\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data[0] / len(input)\n",
    "\n",
    "def train(line):\n",
    "    inputs = str2tensor(line[:-1])\n",
    "    # print(\"inputs size: \", inputs.size())\n",
    "    target = str2tensor(line[1:])\n",
    "    \n",
    "    hidden = decoder.init_hidden()\n",
    "    decoder_in = inputs[0]\n",
    "    \n",
    "    loss = 0\n",
    "    for c in range(len(inputs)):\n",
    "        # print(\"decoder_in size: \", decoder_in.size())\n",
    "        output, hidden = decoder(decoder_in, hidden)\n",
    "        loss += criterion(output, target[c])\n",
    "        decoder_in = output.max(1)[1]\n",
    "    \n",
    "    decoder.zero_grad()\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data[0] / len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2 epochs...\n",
      "[Epoch: (1 50%) Step: (0 0%) loss: 4.8857]\n",
      "WhI\tF{:B4vp 5\u0006m\u0004\u001bmO.\u00138SyB\u0018g\u001a\u001d",
      "75W^Dd^^\u0011\u0006?3>\u000b",
      "\u0015wA5PB3\f",
      "O\u0014P\"\u0017c\u00195)\u0005y~:\u001abWm\"$=\u001e",
      "a#\u0005\u0013ts$#\u001d",
      "d{6@x0nAn7\u0013^\u0014\u000b",
      "\u000fP\\RI^J \n",
      "\n",
      "[Epoch: (1 50%) Step: (100 0%) loss: 3.1914]\n",
      "Whod,ihehedcwoehcgdsclnteehewselnosnieigeogegest,eifhoma,iinanlshdachcslheeeef:hnne:nne,ht:oeeeadoakna \n",
      "\n",
      "[Epoch: (1 50%) Step: (200 0%) loss: 2.9407]\n",
      "Whteehesdlar,ehoeuefofsoteeavhiedamfiiitfhotnui,se,mmdho,ahaauahsakkteconvtohelusetulheytsonhatowehibo \n",
      "\n",
      "[Epoch: (1 50%) Step: (300 0%) loss: 2.9553]\n",
      "Whoeoewiawioiosdnreiolteeoeaeuoaebltdleaamoo,atheeheholhmaoyooy.ecuiygtiyrloaoeaeewewerridnteltkrsaana \n",
      "\n",
      "[Epoch: (1 50%) Step: (400 1%) loss: 3.0961]\n",
      "Whrrnusmpsesegacerneoektyheetis,eomboeoyae,eefeeeheo4adbc,lteci;,etimeeheotdlofissethleeestgeluunh.eoe \n",
      "\n",
      "[Epoch: (1 50%) Step: (500 1%) loss: 2.9875]\n",
      "Whannhheolmbsdiaass,ewrsmsiatioltetott.resitmethtietihybtaflsotwaee,tnmodseys.iietq,kntdyiyeilmikonaes \n",
      "\n",
      "[Epoch: (1 50%) Step: (600 1%) loss: 2.9545]\n",
      "Whoidcugeehescm,toeeyhtlsyotesiapisdokee,ts,diooicsmimtei;woeeooyimeeumseinmnmiystsredirirkcoeeeheoelo \n",
      "\n",
      "[Epoch: (1 50%) Step: (700 2%) loss: 3.0427]\n",
      "Whvnnrtweauhetm,snhguinvmnnaobrd,tntlirptsuchon.fedgirb:agtnph,h,dioy,optstrftienaoayhoaith,a,eaah.bel \n",
      "\n",
      "[Epoch: (1 50%) Step: (800 2%) loss: 3.1415]\n",
      "Whotcainoqrmhfabsl:sanu'ohesadaroooetudicdeeosoemibitmhtashoel,houeleeoorirnoiedehldemh.effrdsteketeue \n",
      "\n",
      "[Epoch: (1 50%) Step: (900 2%) loss: 2.9305]\n",
      "Whoyiatsonrymrseitlb.nrteteoindendni,gteillr,vseohrl'fwidnreestehfo:linidrsrwnnmnoifsunnriuobrkh:etter \n",
      "\n",
      "[Epoch: (1 50%) Step: (1000 3%) loss: 3.0124]\n",
      "Whoaitynoonudtadyromlnotlltoesldelodoeneewivoeatgrteltvtsdhttuohflgeoetllufowhsvvytyynnnhrotefotobrrng \n",
      "\n",
      "[Epoch: (1 50%) Step: (1100 3%) loss: 3.0821]\n",
      "Whsesowdendthsoitedldhcieioroiltwo,t-ueeashstdwtoomelyti:,rerleytnosoeddestiytd?reoufordsiereasf;awiwn \n",
      "\n",
      "[Epoch: (1 50%) Step: (1200 3%) loss: 3.0149]\n",
      "Whntihieetnrht'isnhgeehees,rmilomoondnvewooohsleloledyceeoterauausohaabhemgayih.aoruimrbsir,nitytoemdd \n",
      "\n",
      "[Epoch: (1 50%) Step: (1300 3%) loss: 2.8881]\n",
      "Whulmbbwpkusetcedbeegad,sruholtroeisrectumasushseuea,rmbeir,eohnosirshio:wtegdice:nehusseinsse,nsnseeu \n",
      "\n",
      "[Epoch: (1 50%) Step: (1400 4%) loss: 2.9864]\n",
      "Whisboewtygiesygientnotteeeeinvoni:gdytoryonhsayr,rs,wofrsihacehentgu,duaewoyhrtetgswylo'aelrfltm:,oye \n",
      "\n",
      "[Epoch: (1 50%) Step: (1500 4%) loss: 2.9130]\n",
      "Whieeguiesfteunl.sibenneeuk.ticeuooiouoemenle,,od:winows,efacpewr,feaneevhi,ou.fogcdtnteeshrteohdfetnd \n",
      "\n",
      "[Epoch: (1 50%) Step: (1600 4%) loss: 2.7203]\n",
      "Whrocr:oeviijleoiotnsgouryrto,japthrr,:cruesmyrneuoewoouyemcsneoco,fessretgseoles,es.tdoocot.duthyetee \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# main \n",
    "decoder = RNN(input_size=n_characters, \n",
    "              hidden_size=hidden_size, \n",
    "              output_size=n_characters, \n",
    "              n_layers=n_layers)\n",
    "\n",
    "if t.cuda.is_available():\n",
    "    decoder.cuda()\n",
    "\n",
    "# optimizer and loss function\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = Data.DataLoader(dataset=TextDataset(), \n",
    "                               batch_size=batch_size,\n",
    "                               shuffle=True)\n",
    "print(\"Training for %d epochs...\" % n_epochs)\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    for i, (lines, _) in enumerate(train_loader):\n",
    "        # print(\"i: \", i)\n",
    "        # print(\"lines: \", lines)\n",
    "        \n",
    "        loss = train(lines[0])                   # Batch size is 1\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print('[Epoch: (%d %d%%) Step: (%d %d%%) loss: %.4f]' % \n",
    "                   (epoch, epoch / n_epochs * 100, \n",
    "                    i, i / len(train_loader.dataset) * 100,\n",
    "                    loss))\n",
    "            print(generate(decoder, 'Wh', 100), '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
