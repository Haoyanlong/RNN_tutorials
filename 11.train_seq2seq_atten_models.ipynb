{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable as V\n",
    "import torch.utils.data as Data\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from pyfile.text_loader import TextDataset\n",
    "import pyfile.seq2seq_models as sm\n",
    "from pyfile.seq2seq_models import cuda_variable, str2tensor, EOS_token, SOS_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_LAYERS = 1\n",
    "BATCH_SIZE = 1\n",
    "N_EPOCH = 2\n",
    "N_CHARS = 128      # ASCII\n",
    "HIDDEN_SIZE = N_CHARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train for a given src and target\n",
    "def train(src, target):\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    src_var = str2tensor(src)\n",
    "    # print(\"src_var: \", src_var)\n",
    "    target_var = str2tensor(target, eos=True)   # Add the EOS token\n",
    "    # print(\"target_var: \", target_var)\n",
    "    \n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(src_var, encoder_hidden)\n",
    "    \n",
    "    hidden = encoder_hidden\n",
    "    \n",
    "    for c in range(len(target_var)):\n",
    "        # First, we feed SOS. Others, we use teacher forcing.\n",
    "        token = target_var[c - 1] if c else str2tensor(SOS_token)\n",
    "        output, hidden, attention = decoder(token, hidden, encoder_outputs)\n",
    "        \n",
    "        loss += criterion(output, target_var[c])\n",
    "    \n",
    "    encoder.zero_grad()\n",
    "    decoder.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.data[0] / len(target_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simple test to show how train works\n",
    "def test():\n",
    "    encoder_test = sm.EncoderRNN(10, 10, 2)\n",
    "    decoder_test = sm.AttnDecoderRNN(10, 10, 2)\n",
    "    \n",
    "    if t.cuda.is_available():\n",
    "        encoder_test.cuda()\n",
    "        decoder_test.cuda()\n",
    "    \n",
    "    encoder_hidden = encoder_test.init_hidden()\n",
    "    word_input = cuda_variable(t.LongTensor([1, 2, 3]))\n",
    "    encoder_outputs, encoder_hidden = encoder_test(word_input, encoder_hidden)\n",
    "    print(\"encoder_outputs size: \", encoder_outputs.size())\n",
    "    \n",
    "    word_target = cuda_variable(t.LongTensor([1, 2, 3]))\n",
    "    decoder_attns = t.zeros(1, 3, 3)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    for c in range(len(word_target)):\n",
    "        decoder_output, decoder_hidden, decoder_attn = \\\n",
    "               decoder_test(word_target[c], decoder_hidden, encoder_outputs)\n",
    "        print(\"decoder output size: \", decoder_output.size(), \n",
    "              \"\\ndecoder hidden size: \", decoder_hidden.size(),\n",
    "              \"\\ndecoder attn size: \", decoder_attn.size())\n",
    "        decoder_attns[0, c] = decoder_attn.squeeze(0).cpu().data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Traslate the given input\n",
    "def translate(enc_input=\"thisissungkim.iloveyou\", predict_len=100, temperate=0.9):\n",
    "    input_var = str2tensor(enc_input)\n",
    "    \n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_var, encoder_hidden)\n",
    "    \n",
    "    hidden = encoder_hidden\n",
    "    \n",
    "    predicted = ''\n",
    "    \n",
    "    dec_input = str2tensor(SOS_token)\n",
    "    attentions = []\n",
    "    \n",
    "    for c in range(predict_len):\n",
    "        output, hidden, attention = decoder(dec_input, hidden, encoder_outputs)\n",
    "        \n",
    "        # Sample from the nerwork as a multi nominal distribution\n",
    "        output_dist = output.data.view(-1).div(temperate).exp()\n",
    "        top_i = t.multinomial(output_dist, 1)[0]\n",
    "        attentions.append(attention.view(-1).data.cpu().numpy().tolist())\n",
    "        \n",
    "        if top_i is EOS_token:\n",
    "            break\n",
    "        \n",
    "        predicted_char = chr(top_i)\n",
    "        predicted += predicted_char\n",
    "        \n",
    "        dec_input = str2tensor(predicted_char)\n",
    "    return predicted, attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# main \n",
    "# N_CHARS: 128, HIDDEN_SIZE: 128, N_LAYERS: 1\n",
    "encoder = sm.EncoderRNN(N_CHARS, HIDDEN_SIZE, N_LAYERS)\n",
    "decoder = sm.AttnDecoderRNN(HIDDEN_SIZE, N_CHARS, N_LAYERS)\n",
    "\n",
    "if t.cuda.is_available():\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "print(\"encoder: \", encoder,\n",
    "      \"\\ndecoder: \", decoder)\n",
    "\n",
    "\n",
    "# Optimizer and Loss\n",
    "params = list(encoder.parameters()) + list(decoder.parameters())\n",
    "optimizer = optim.Adam(params, lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = Data.DataLoader(dataset=TextDataset(),\n",
    "                               batch_size=BATCH_SIZE,\n",
    "                               shuffle=True,\n",
    "                               num_workers=2)\n",
    "\n",
    "print(\"Training for %d epochs...\" % N_EPOCH)\n",
    "\n",
    "for epoch in range(1, N_EPOCH + 1):\n",
    "    # Get srcs and targets from data loader\n",
    "    for i, (srcs, targets) in enumerate(train_loader):\n",
    "        # print(\"srcs[0]: \", srcs[0],\n",
    "        #       \"\\ntargets[0]: \", targets[0])\n",
    "    \n",
    "        train_loss = train(srcs[0], targets[0])\n",
    "        \n",
    "        if i % 100 is 0:\n",
    "            print(\"Epoch: (%d/%d) Step: (%d/%d) Loss: %.4f\" %\n",
    "                  (epoch, N_EPOCH, i, len(train_loader), train_loss))\n",
    "            \n",
    "            output, _ = translate(srcs[0])\n",
    "            print(srcs[0], output, '\\n')\n",
    "            \n",
    "            output, attentions = translate()\n",
    "            print('thisissungkim.iloveyou.', output, '\\n')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for eachdata in train_loader.dataset[:10]:\n",
    "    print(eachdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('beforeweproceedanyfurther,hearmespeak.', 'Before we proceed any further, hear me speak.')\n"
     ]
    }
   ],
   "source": [
    "print(train_loader.dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "srcs size:  theplebeianshavegotyourfellow-tribune\n",
      "targets size:  The plebeians have got your fellow-tribune\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4ce7fe2fd88f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"srcs size: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrcs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"targets size: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, (srcs, targets) in enumerate(train_loader):\n",
    "    print(\"srcs: \", srcs[0])\n",
    "    print(\"targets: \", targets[0])  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
