{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import time\n",
    "import math\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable as V\n",
    "import torch.utils.data as Data\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from pyfile.text_loader import TextDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return list(itertools.chain.from_iterable(l))\n",
    "\n",
    "seqs = ['ghatmasala', 'nicela', 'chutpakodas']\n",
    "\n",
    "# make <pad> idx 0\n",
    "vocab = ['<pad>'] + sorted(list(set(flatten(seqs))))\n",
    "\n",
    "# make model\n",
    "embedding_size = 3\n",
    "embed = nn.Embedding(len(vocab), embedding_size)\n",
    "lstm = nn.LSTM(embedding_size, 5)\n",
    "\n",
    "vectorized_seqs = [[vocab.index(tok) for tok in seq] for seq in seqs]\n",
    "print(\"vectorized_seqs: \", vectorized_seqs)\n",
    "\n",
    "print(\"lengths: \", [x for x in map(len, vectorized_seqs)])\n",
    "# get the length of each seq in your batch\n",
    "seq_lengths = t.LongTensor([x for x in map(len, vectorized_seqs)])\n",
    "\n",
    "# dump padding everywhere, and place seqs on the left\n",
    "# NOTE: only need a tensor as big as longest sequence\n",
    "seq_tensor = V(t.zeros(len(vectorized_seqs, seq_lengths.max()))).long()\n",
    "for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "    seq_tensor[idx, :seqlen] = t.LongTensor(seq)\n",
    "\n",
    "print(\"seq_tensor: \", seq_tensor)\n",
    "\n",
    "# sort tensors by length!\n",
    "seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
    "seq_tensor = seq_tensor[perm_idx]\n",
    "print(\"seq_tensor after sorting: \", seq_tensor)\n",
    "\n",
    "# utils.rnn lets give (B, L, D) \n",
    "# tensors where B is the batch size, L is the max length,\n",
    "# if use batch_first=True\n",
    "# Otherwise, give (L, B, D) tensors\n",
    "seq_tensor = seq_tensor.transpose(0, 1)   # (B, L, D) -> (L, B, D)\n",
    "print(\"seq_tensor after transposing\", seq_tensor.size(), seq_tensor.data)\n",
    "\n",
    "# embed sequences\n",
    "embeded_seq_tensor = embed(seq_tensor)\n",
    "print(\"seq_tensor after embedding\", embeded_seq_tensor.size(), seq_tensor.data)\n",
    "\n",
    "# pack them nicely\n",
    "packed_input = pack_padded_sequence(embeded_seq_tensor, seq_lengths.cpu().numpy())\n",
    "\n",
    "# throw them through LSTM (remember to give batch_first=True) here\n",
    "# if packed with if you packed with it)\n",
    "packed_output, (ht, ct) = lstm(packed_input)\n",
    "\n",
    "# unpack your output if required\n",
    "output, _ = pad_packed_sequence(packed_output)\n",
    "print(\"Lstm output: \", output.size(), output.data)\n",
    "\n",
    "# Or if you just want the final hidden state\n",
    "print(\"Last output: \", ht[-1].size, ht[-1].data)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
